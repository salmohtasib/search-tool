
import streamlit as st
from googlesearch import search as gsearch
from duckduckgo_search import ddg
import requests
from bs4 import BeautifulSoup
import re
import tldextract
import openai

openai.api_key = st.secrets["OPENAI_API_KEY"]

def expand_keywords(keyword):
    prompt = f"Ø§ÙƒØªØ¨ Ù„ÙŠ 5 ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© Ù…Ø±ØªØ¨Ø·Ø© Ø¨Ù€ '{keyword}'"
    resp = openai.Completion.create(model="gpt-3.5-turbo", prompt=prompt, max_tokens=50)
    return [kw.strip() for kw in resp.choices[0].text.split('\n') if kw.strip()]

def fetch_urls(keyword):
    urls = set()
    for q in [keyword] + expand_keywords(keyword):
        for u in gsearch(q, num_results=5, lang="ar"):
            urls.add(u)
        for result in ddg(q, max_results=5):
            urls.add(result['href'])
    return list(urls)

def extract_from_page(url):
    try:
        r = requests.get(url, timeout=5)
        soup = BeautifulSoup(r.text, "html.parser")
        text = soup.get_text(" ", strip=True)
        phones = re.findall(r'05\d{8}', text)
        emails = re.findall(r'[\w\.-]+@[\w\.-]+', text)
        wa = re.findall(r'https://wa\.me/\d+', text)
        domain = tldextract.extract(url).domain
        name = soup.title.string if soup.title else domain
        return {"Ø§Ø³Ù… Ø§Ù„Ù…ØªØ¬Ø±": name, "Ø§Ù„Ø±Ø§Ø¨Ø·": url, "Ø¬ÙˆØ§Ù„": phones[0] if phones else "", 
                "Ø¥ÙŠÙ…ÙŠÙ„": emails[0] if emails else "", "ÙˆØ§ØªØ³Ø§Ø¨": wa[0] if wa else ""}
    except:
        return None

st.set_page_config(page_title="Ø£Ø¯Ø§Ø© Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©", layout="centered")
st.title("ğŸ” Ø£Ø¯Ø§Ø© Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø°ÙƒÙŠØ© â€“ Ø§Ù„Ù†Ø³Ø®Ø© 1.5")

st.markdown("### Ø§Ù„Ù…ÙØ§ØªÙŠØ­")
keyword = st.text_input("ğŸ”‘ Ø£Ø¯Ø®Ù„ Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©")
custom_sites = st.text_area("ğŸŒ Ù…ÙˆØ§Ù‚Ø¹ Ø¥Ø¶Ø§ÙÙŠØ© (ÙˆØ§Ø­Ø¯Ø© ÙÙŠ Ø§Ù„Ø³Ø·Ø±)")
if st.button("ğŸ” Ø§Ø¨Ø­Ø«"):
    if not keyword:
        st.warning("Ø£Ø¯Ø®Ù„ ÙƒÙ„Ù…Ø© Ù…ÙØªØ§Ø­ÙŠØ© Ø£ÙˆÙ„Ø§Ù‹")
    else:
        st.info("...Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¨Ø­Ø« ÙˆØ¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
        urls = fetch_urls(keyword)
        for s in custom_sites.splitlines():
            if s.startswith("http"):
                urls.append(s.strip())
        data = []
        for u in urls[:15]:
            res = extract_from_page(u)
            if res: data.append(res)
        st.write(f"ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ {len(data)} ØµÙ")
        st.dataframe(data)
